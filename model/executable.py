# -*- coding: utf-8 -*-
"""single_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J9u7hweBw5TPyIZbmeUVaf1BAWipeVj0
"""
import subprocess
import sys

message = sys.argv[1]

PATH_Camembert = "./model/camembert_model.pth"

from pathlib import Path
PATH_Spacy = Path("./model/spacy_ner_lg.pkl")

def install(package):
    subprocess.check_call([sys.executable, "-m", "pip", "install", package])

install("transformers")
install("sentencepiece")
install("pandas")
install("numpy")
install("nltk")
install("scikit-learn")
install("torch")
install("spacy")
install("pays")

import transformers
import sentencepiece
import pandas as pd
import numpy as np
from os import listdir
from tqdm import tqdm, trange
import textwrap
import nltk
import random
import re
import sys
from sklearn.preprocessing import LabelEncoder
import torch
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
import torch.nn as nn
from transformers import CamembertTokenizer, CamembertForSequenceClassification, AdamW
from transformers import AutoModelForSequenceClassification, AutoTokenizer, CamembertConfig, CamembertModel
from transformers import AutoConfig
from transformers import AutoModelForSequenceClassification
from transformers import logging
from transformers.models.roberta.modeling_roberta import RobertaClassificationHead
from keras_preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from torch.nn import MSELoss, CrossEntropyLoss, BCEWithLogitsLoss
from transformers.modeling_outputs import SequenceClassifierOutput
import pickle 
from pays import Countries
import spacy             

longueur_max = 512
batch_size = 12
epochs = 4
learning_rate = 2e-5
encodeur = LabelEncoder()

#we get rid of stopwords because of maximum sequence length for camembert_base model
import string
from nltk.corpus import stopwords
nltk.download('stopwords')
stopwordsRegex = '(?:^|(?<= ))('+"|".join(stopwords.words('french'))+')(?:(?= )|$)'
punctuationRegex = '['+string.punctuation+']'

nlp = spacy.load("fr_core_news_lg")

def remove(txt, regex):
  res = re.sub(regex,' ',txt)
  return res

def clean(sentence):
  sentence = str(sentence)
  sentence = remove(sentence, '\s+')
  sentence = remove(sentence, '[…|°|;|×|/|-]')
  sentence = remove(sentence, punctuationRegex)
  sentence = remove(sentence, stopwordsRegex)
  sentence = sentence.lower()
  return sentence

def return_lemma(sentence):
    doc = nlp(sentence)
    return [X.lemma_ for X in doc if str(X).isspace()==False]

def clean_no_lower(sentence):
  sentence = str(sentence)
  sentence = remove(sentence, '\s+')
  sentence = remove(sentence, '[…|°|;|×|/|-]')
  sentence = remove(sentence, punctuationRegex)
  sentence = remove(sentence, stopwordsRegex)
  return sentence

#import du modèle
nom_modele = "camembert-base"
camembert_tokenizer = CamembertTokenizer.from_pretrained(nom_modele, do_lower_case=True)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
_model_type = 'roberta'
config = CamembertConfig.from_pretrained(nom_modele)
config.num_labels = 4
add_pooler = True
reinit_pooler = True

class Net(nn.Module):
    def __init__(self, config, nom_modele, add_pooler):
        super(Net, self).__init__()
        self.num_labels = config.num_labels
        self.config = config

        self.roberta = CamembertModel.from_pretrained(nom_modele, add_pooling_layer=add_pooler)
        self.classifier = RobertaClassificationHead(config)
        
    def forward(self, input_ids, attention_mask, token_type_ids=None, labels=None, return_dict=None, output_attentions=None, output_hidden_states=None):
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        outputs = self.roberta(
            input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states
        )
        sequence_output = outputs[0]
        logits = self.classifier(sequence_output)

        loss = None
        if labels is not None:
            if self.config.problem_type is None:
                if self.num_labels == 1:
                    self.config.problem_type = "regression"
                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):
                    self.config.problem_type = "single_label_classification"
                else:
                    self.config.problem_type = "multi_label_classification"

            if self.config.problem_type == "regression":
                loss_fct = MSELoss()
                if self.num_labels == 1:
                    loss = loss_fct(logits.squeeze(), labels.squeeze())
                else:
                    loss = loss_fct(logits, labels)
            elif self.config.problem_type == "single_label_classification":
                loss_fct = CrossEntropyLoss()
                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
            elif self.config.problem_type == "multi_label_classification":
                loss_fct = BCEWithLogitsLoss()
                loss = loss_fct(logits, labels)

        if not return_dict:
            output = (logits,) + outputs[2:]
            return ((loss,) + output) if loss is not None else output

        return SequenceClassifierOutput(
            loss=loss,
            logits=logits,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )
        
camembert_model = Net(config, nom_modele, add_pooler)

if reinit_pooler:
    print('Reinitialisation des poids du Pooler ...')
    encoder_temp = getattr(camembert_model, _model_type)
    encoder_temp.pooler.dense.weight.data.normal_(mean=0.0, std=encoder_temp.config.initializer_range)
    encoder_temp.pooler.dense.bias.data.zero_()
    for p in encoder_temp.pooler.parameters():
        p.requires_grad = True
    print('Pooler réinitialisé.')

camembert_model.to(device)

camembert_model = torch.load(PATH_Camembert, map_location=torch.device('cpu'))

def decode(cellule):
  if cellule == 0 :
    cellule = False
  if cellule == 1 :
    cellule = True
  return cellule

def single_prediction(message,encodeur):
  # Tokenisation de validation data
  validation_tokenized_ids = [camembert_tokenizer.encode(message, add_special_tokens=True,truncation=True, max_length=longueur_max)]

  # Padding 
  validation_tokenized_ids = pad_sequences(validation_tokenized_ids, maxlen=longueur_max, dtype="long", padding="post")

  # Masque d'attention pour ne pas prendre en compte les paddings
  masques_attentions = []
  # Un masque avec des 1 pour chaque token et des 0 pour les paddings
  for article_seq in validation_tokenized_ids:
      seq_masque = [float(i>0) for i in article_seq]  
      masques_attentions.append(seq_masque)

  prediction_inputs = torch.tensor(validation_tokenized_ids)
  prediction_masques = torch.tensor(masques_attentions)
  # Application du modèle 
  predictions = []
  with torch.no_grad():
     # Passage et calcul des predictions
     outputs =  camembert_model(prediction_inputs.to(device),token_type_ids=None, attention_mask=prediction_masques.to(device))
     logits = outputs[0]
     logits = logits.detach().cpu().numpy() 
     predictions.extend(np.argmax(logits, axis=1).flatten())
  #df["prediction"] = predictions
  #df["prediction"] = df['prediction'].apply(decode)
  return decode(predictions[0])

final_pred = single_prediction(message,encodeur)

regex_dict = {
    "phone_regex":'(?:(?:\+|00)33[\s.-]{0,3}(?:\(0\)[\s.-]{0,3})?|0)[1-9](?:(?:[\s.-]?\d{2}){4}|\d{2}(?:[\s.-]?\d{3}){2})',
    "email_regex" : r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,3}\b',
    "age_regex" : '[0-9]{1}?[0-9]{1} ?ans?',
    "vital_regex": '[12] ?[0-9]{2} ?0[1-9]{1} ?[0-9]{2} ?[0-9]{3} ?[0-9]{3} ?[0-9]{2}',
    "passport_regex" : '[0-9]{2}[A-Z]{2}[0-9]{5}',
    "rib_regex": '[A-Z]{2} ?[0-9]{2} ?[0-9]{4} ?[0-9]{4} ?[0-9]{4} ?[0-9]{4} ?[0-9]{2}'
}

def flag_regex(message, regex, flag_type):
  regex_matched = re.findall(regex, message)
  tag_dict = {}
  tag_dict[flag_type] = regex_matched
  return tag_dict

final_dict={}
for regex in regex_dict.keys():
  final_dict.update(flag_regex(message,regex_dict[regex], regex.split("_regex")[0]))

final_dict["toHide"]=final_pred


countries = Countries()
pays = []
for country in countries:
  pays.append(str(country).lower())


tag_dict = {
  "nationality": pays,
  "situation_familiale": ['marié', 'pacsé', 'divorcé', 'séparé', 'célibataire', 'veuf', 'mariage', 'divorce','marier','divorcer'],
  "orientation_sexuelle": ['hétérosexualité', 'homosexualité','bisexualité', 'asexualité','hétérosexuel','homosexuel','bisexuel','asexuel','hétérosexual','homosexual','bisexual','asexual'],
  "information_medicale": ['endométriose',
 'fibromyalgie',
 'mononucléose',
 'psoriasis',
 'scarlatine',
 'hémorroïdes',
 'schizophrénie',
 'zona',
 'appendicite',
 'burn-out',
 'rougeole',
 'varicelle',
 'gale',
 'chlamydia',
 'diarrhée',
 'lupus',
 'orgelet',
 'phlébite',
 'sciatique',
 'méningite',
 'dépression',
 'avc',
 'acouphènes',
 'anémie',
 'arthrose',
 'autisme',
 'conjonctivite',
 'cystite',
 'diabète',
 'eczéma',
 'grippe',
 'hypothyroïdie',
 'lumbago',
 'papillomavirus',
 'roséole',
 'syphilis',
 'pancréatite',
 'impétigo',
 'algodystrophie',
 'angine',
 'constipation',
 'cruralgie',
 'hyperthyroïdie',
 'pneumopathie',
 'sinusite',
 'tachycardie',
 'tuberculose',
 'urticaire',
 'cholestérol',
 'herpès',
 'ait',
 'bronchite',
 'cancer',
 'chalazion',
 'coqueluche',
 'drépanocytose',
 'erysipèle',
 'escarre',
 'furoncle',
 'glaucome',
 'infarctus',
 'leucémie',
 'mucoviscidose',
 'pneumonie',
 'pneumothorax',
 'rhinopharyngite',
 'septicémie',
 'sida',
 'tendinite',
 'toxoplasmose',
 'vitiligo',
 'ostéoporose',
 'trachéite',
 'bpco',
 'cataracte',
 'fibrome',
 'hypertension',
 'laryngite',
 'lombalgie',
 'mycose',
 'paludisme',
 'phimosis',
 'polyarthrite',
 'polype',
 'purpura',
 'ulcère'],
  "information_religieuse": ['Jaïnisme','Hindouisme','Bouddhisme','Zoroastrisme','taoïsme','Judaïsme','Confucianisme','Mithraïsme','Christianisme','Mandéisme','Manichéisme','Elkasaïsme','Islam','hindou','chrétien','juif','juive','bouddhiste','musulman']
}

def tag_lemmatize_with_dict(message,dict):
  message = clean_no_lower(message)
  message = message.strip()
  message = re.sub("[ ]{2,}", " ", message)
  message = message.split(" ")
  message_lemma = return_lemma(clean(message))
  print(message_lemma)
  output = {'nationality':[],
            'situation_familiale':[],
            'orientation_sexuelle':[], 
            'information_medicale':[],
            'information_religieuse':[]
            }
  for key in dict.keys():
    matched_list=[]
    for word in message_lemma:
      if word in dict[key]:
        matched_list.append(message[message_lemma.index(word)])
        output[key]=matched_list
  return output

outputTags = tag_lemmatize_with_dict(message, tag_dict)

for key in outputTags.keys():
    final_dict[key] = outputTags[key]



def return_NER(sentence,modele):
    # Tokeniser la phrase
    doc = modele(sentence)
    # Retourner le texte et le label pour chaque entité
    return [(X.text, X.label_) for X in doc.ents]

#Lecture du modèle 
with open(PATH_Spacy, 'rb') as file:
    modele_spacy = pickle.load(file)

output_spacy=return_NER(message, modele_spacy)

for o in output_spacy:
    final_dict[o[1]]=o[0]

final_dict= str(final_dict)

print("$FinaleResult-" + final_dict + "$")
sys.stdout.flush()
